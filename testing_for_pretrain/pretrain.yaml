# Pretraining configuration for nanoPLM

model:
  hidden_size: 1024
  intermediate_size: 2048
  num_hidden_layers: 16
  num_attention_heads: 16
  vocab_size: 29
  mlp_activation: swiglu
  mlp_dropout: 0.0
  mlp_bias: false
  attention_bias: false
  attention_dropout: 0.0
  classifier_activation: gelu

training:
  # Dataset
  # Note: these paths are RELATIVE to where you RUN the command NOT the YAML file.
  train_fasta: output/data/split/train.fasta
  val_fasta: output/data/split/val.fasta

  # Training hyperparameters
  max_length: 1024
  batch_size: 32
  num_epochs: 10
  learning_rate: 3e-6
  weight_decay: 0.0
  warmup_ratio: 0.05
  mlm_probability: 0.3
  gradient_accumulation_steps: 1
  eval_steps: 100
  save_steps: 1000
  seed: 42
  mask_replace_prob: 0.8
  random_token_prob: 0.1
  leave_unchanged_prob: 0.1
